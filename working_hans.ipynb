{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import keras.models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__load in the text data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hans=(open('Hans_3').read())\n",
    "hans=hans.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove double spaces and line endings to avoid counting\n",
    "hans = hans.replace(\"\\n\", \" \")\n",
    "hans = hans.replace(\"  \", \" \")\n",
    "hans = hans.replace(\"[illustration: _\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create dictionaries to map char to num and then num to char for character encoding and decoding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the unique characters that appear\n",
    "hans_characters = sorted(list(set(hans)))\n",
    "# map the unique characters to a dictionary with char as key and len of set list as value\n",
    "hans_n_to_char = {n:char for n, char in enumerate(hans_characters)}\n",
    "# map the unique characters to a dictionary with len of set list as key and char as value\n",
    "hans_char_to_n = {char:n for n, char in enumerate(hans_characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create lists of 100 chars as sequences to feed to the model for predicting the next char__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of characters\n",
    "hans_X = []\n",
    "hans_Y = []\n",
    "length = len(hans)\n",
    "seq_length = 100\n",
    "for i in range(0, length-seq_length, 1):\n",
    "    sequence = hans[i:i + seq_length]\n",
    "    label = hans[i + seq_length]\n",
    "    hans_X.append([hans_char_to_n[char] for char in sequence])\n",
    "    hans_Y.append(hans_char_to_n[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__reshape the data [samples, time steps, features] expected by an LSTM network__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hans_X_modified = np.reshape(hans_X, (len(hans_X), seq_length, 1))\n",
    "# normalize the X data\n",
    "hans_X_modified = hans_X_modified / float(len(hans_characters))\n",
    "# one hot encode the output Y variable \n",
    "hans_Y_modified = np_utils.to_categorical(hans_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__build the sequential LSTM model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hans_model = Sequential()\n",
    "hans_model.add(LSTM(700, input_shape=(hans_X_modified.shape[1], hans_X_modified.shape[2]), \n",
    "               return_sequences=True))\n",
    "hans_model.add(Dropout(0.2))\n",
    "hans_model.add(LSTM(700))\n",
    "hans_model.add(Dropout(0.2))\n",
    "hans_model.add(Dense(hans_Y_modified.shape[1], activation='softmax'))\n",
    "hans_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__save epoch checkpoint weights__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint, do this before fitting but not needed unless you intend \n",
    "# to fit the model below\n",
    "\n",
    "# filepath=\"hans-weights-{epoch:02d}-{loss:.4f}.new\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "#                              save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__fit the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed for this demo\n",
    "#hans_model.fit(hans_X_modified, hans_Y_modified, epochs=11, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__load in trained weights from previous model epoch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"hans-weights-07-1.3190.hdf5\"\n",
    "hans_model.load_weights(filename)\n",
    "hans_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__GENERATE THE STORY from a randomly chosen seed from corpus text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(hans_characters)\n",
    "\n",
    "start = np.random.randint(0, len(hans_X)-1)\n",
    "pattern = hans_X[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([hans_n_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(200):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = hans_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = hans_n_to_char[index]\n",
    "    seq_in = [hans_n_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__to pickle for flask app or other use__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hans_model.save('simple_hans.hd5')\n",
    "# pickle.dump(hans_n_to_char, open(\"hans_n_to_char.pkl\", \"wb\"))\n",
    "# pickle.dump(hans_char_to_n, open(\"hans_char_to_n.pkl\", \"wb\"))\n",
    "# pickle.dump(hans_X, open(\"hans_X.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
